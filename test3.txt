# Linear Regression
# Regression is the term used when attempting to determine the
# relationship between variables. That relationship is used to
# forecast the result of upcoming events in statistical modeling
# and machine learning.
# Linear regression uses the relationship between the data-points
# to draw a straight line through all of them. This line can be used
# to predict future values.
# How Does it Work?
# Python has methods for finding a relationship between data-points and to draw a line of
# linear regression. We will show you how to use these methods instead of going through
# the mathematic formula.
# y = ax + b
# y = slope * input + intercept
# slope, intercept, r, p, std_err = stats.linregress(x, y)
# EXAMPLE:


import matplotlib.pyplot as plt
from scipy import stats
grade = [90,86,87,88,98,86,97,87,94,78,67,84,76]
student = [6,7,9,7,3,17,2,9,5,11,12,9,8]
slope, intercept, r, p, std_err = stats.linregress(grade, student)
def myfunc(grade):
 return slope * grade + intercept
mymodel = list(map(myfunc, grade))
plt.scatter(grade, student)
plt.plot(grade, mymodel)
plt.show()
print(r)
print(p)
print(std_err)
pred = myfunc(30)
print(pred)


# Polynomial Regression
# It might be suitable for polynomial regression if
# your data points plainly do not fit a linear
# regression (a straight line through all of your data
# points).
# Similar to linear regression, polynomial regression
# seeks the optimal path across the data points by
# utilizing the relationship between the variables x
# and y.
# EXAMPLE:
import numpy
import matplotlib.pyplot as plt
x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,20,21]
y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]
mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))
myline = numpy.linspace(1, 21, 100)
plt.scatter(x, y)
plt.plot(myline, mymodel(myline))
plt.show()



# R-Squared
# The strength of the correlation between the x- and y-axis values is crucial since polynomial
# regression cannot predict anything if there is no correlation. The r squared number is used
# to evaluate the strength of the relationship. The range of the r-squared value is 0 to 1,
# with 0 denoting no relationship and 1 denoting perfect correlation.
import numpy
from sklearn.metrics import r2_score
x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,20,21]
y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]
mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))
print(r2_score(y, mymodel(x)))